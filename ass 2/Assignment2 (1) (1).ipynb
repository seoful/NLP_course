{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norvig's spelling corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MoQeEsZvHvvi"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# downloading big.txt and saving it\n",
    "# big.txt is a file used in the norvig's solution\n",
    "\n",
    "#response = requests.get(\"https://norvig.com/big.txt\")\n",
    "#if response.status_code == 200:\n",
    "#    with open(\"big.txt\", 'wb') as f:\n",
    "#        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norvig's spelling corrector - copied from https://norvig.com/spell-correct.html\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def P_norvig(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P_norvig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norvig_correction(\"speling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom spelling corrector based on Norvig's + bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# loading bigrams and their counts from 'bigrams.txt'\n",
    "BIGRAMS = defaultdict(Counter)\n",
    "with open('bigrams.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        count, word1, word2 = line.split()\n",
    "        BIGRAMS[word1][word2] = int(count)\n",
    "        \n",
    "def prev_bigram_probability(prev_word, candidate):\n",
    "    return BIGRAMS[prev_word][candidate] / sum(BIGRAMS[prev_word].values())\n",
    "\n",
    "def next_bigram_probability(candidate, next_word):\n",
    "    return sum(BIGRAMS[word][next_word] for word, next_word_counter in BIGRAMS.items() if next_word in next_word_counter) / BIGRAMS[candidate][next_word]\n",
    "        \n",
    "def P_bigram(candidate, prev_word=None, next_word=None): # calculate conditional probability of a candidate word given prev_word and next_word\n",
    "    if prev_word is not None and next_word is not None:\n",
    "        return prev_bigram_probability(prev_word, candidate) * next_bigram_probability(candidate, next_word)\n",
    "    elif prev_word is not None:\n",
    "        return prev_bigram_probability(prev_word, candidate)\n",
    "    elif next_word is not None:\n",
    "        return next_bigram_probability(candidate, next_word)\n",
    "    else:\n",
    "        return WORDS[candidate] / sum(WORDS.values())\n",
    "    \n",
    "def prev_bigram_exists(prev_word, candidate):\n",
    "    return prev_word is not None and prev_word in BIGRAMS and candidate in BIGRAMS[prev_word]\n",
    "\n",
    "def next_bigram_exists(candidate, next_word):\n",
    "    return next_word is not None and candidate in BIGRAMS and next_word in BIGRAMS[candidate]\n",
    "\n",
    "def bigram_correction(word, prev_word=None, next_word=None): # most probable spelling correction for word given prev_word and next_word\n",
    "    correction_candidates = candidates(word)\n",
    "    \n",
    "    filtered_candidates = [candidate for candidate in correction_candidates\n",
    "                           if prev_bigram_exists(prev_word, candidate) \n",
    "                           and next_bigram_exists(candidate, next_word)]\n",
    "    \n",
    "    if len(filtered_candidates) > 0:\n",
    "        return max(filtered_candidates, key=lambda candidate: P_bigram(candidate, prev_word, next_word))\n",
    "    \n",
    "    filtered_candidates = [candidate for candidate in correction_candidates\n",
    "                           if prev_bigram_exists(prev_word, candidate) \n",
    "                           or next_bigram_exists(candidate, next_word)]\n",
    "    \n",
    "    if len(filtered_candidates) > 0:\n",
    "        return max(filtered_candidates, key=lambda candidate: P_bigram(candidate, \n",
    "                                                                prev_word if prev_bigram_exists(prev_word, candidate) else None, \n",
    "                                                                next_word if next_bigram_exists(candidate, next_word) else None))\n",
    "    \n",
    "    return max(correction_candidates, key=P_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'king'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_correction(\"dking\", next_word=\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doing'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_correction(\"dking\", next_word=\"homework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrector(sentence, method=\"bigram\"):\n",
    "    words = sentence.split()\n",
    "    corrections = []\n",
    "    for i in range(len(words)):\n",
    "        if not words[i].isalpha(): # if string contains punctuation then leave it as it is - it is not a word\n",
    "            corrections.append(words[i])\n",
    "            continue\n",
    "        if method == \"norvig\":\n",
    "            corrections.append(norvig_correction(words[i]))\n",
    "        else: #if method = \"bigram\"\n",
    "            if i != 0:\n",
    "                prev_word = words[i-1]\n",
    "                if not prev_word.isalpha():\n",
    "                    prev_word = None\n",
    "            else:\n",
    "                prev_word = None\n",
    "            if i != len(words) - 1:\n",
    "                next_word = words[i+1]\n",
    "                if not next_word.isalpha():\n",
    "                    next_word = None\n",
    "            else:\n",
    "                next_word = None\n",
    "            corrections.append(bigram_correction(words[i], prev_word=prev_word, next_word=next_word))\n",
    "    return ' '.join(corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Norvig's spelling corrector as a basis of my solution. To obtain vocabulary of words and their frequencies, I used big.txt, as in the Norvig's solution.\n",
    "\n",
    "I used bigrams dataset specified in the assignment description. Larger datasets are harder to obtain: the ones I found were not free to use. Also, working with them would cost more computation power.\n",
    "\n",
    "My solution has the following implementation:\n",
    "\n",
    "First, I generate possible correction candidates that are 1 (or 2, if there are none) edits away from the original word that are in the vocabulary from big.txt (as in the Norvig's solution).\n",
    "\n",
    "Then, for each candidate, I check if both bigrams prev_word+candidate and candidate+next_word exist (the word is not in the very beginning or very end of the sentence) and are present in the bigrams dataset.\n",
    "\n",
    "Then, I take candidates for which both these bigrams exist and are present in the dataset and calculate conditional probability of each candidate appearing in a given context: P(candidate|prev_word,next_word) = P(candidate|prev_word) * P (candidate|next_word) = count of bigrams (prev_word, candidate) / count of all bigrams (prev_word, smth) * count of bigrams(candidate, next_word) / count of all bigrams (smth, next_word). I return the word with the highest calculated conditional probability.\n",
    "\n",
    "If there are no candidates for which both the bigrams exist and are present in the dataset, I take the candidates for which one of the bigrams exists and is present in the dataset, calculate conditional probability of them appearing in a context with it the same way, then rank them and return the one with the highest calculated colditional probability.\n",
    "\n",
    "If for every candidate there are no bigrams that exist and are present in the dataset, I rank the candidates by the frequency of their usage, as in the Norvig's solution, and return the one with the highest one.\n",
    "\n",
    "To test my solution, I used collection of texts 'brown' from nltk. I randomly took 150 sentences from there, for each of them I generated a misspelled version, and then I ran them through Norvig's corrector and the one I wrote, and for each calculated the accuracy of correction. It appeared my solution that used bigrams got almost the same results as the Norvig's one - I think adding more context via using 3grams, 4grams, ... and larger datasets could help with that - using small bigrams dataset has its limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "# downloading dataset containing english texts\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "test_set_size = 150\n",
    "sentence_list = [' '.join(sentence).lower() for sentence in sentences]\n",
    "random_sentences = random.sample(sentence_list, test_set_size)\n",
    "\n",
    "noise = 0.15\n",
    "def generate_misspelling(word):\n",
    "    # if input string contains punctuation then leave it as it is - it is not a word\n",
    "    if not word.isalpha():\n",
    "        return word\n",
    "    if random.random() > noise:\n",
    "        return word\n",
    "    # select one of the edit functions randomly - edit1 with 80% probability\n",
    "    if random.random() < 0.8:\n",
    "        edit_function = edits1\n",
    "    else:\n",
    "        edit_function = edits2\n",
    "    # apply the selected edit function to the word and return the result\n",
    "    return random.choice(list(edit_function(word)))\n",
    "\n",
    "test_set = []\n",
    "for sentence in random_sentences:\n",
    "    # apply random edits to randomly chosen words in the sentence to generate spelling errors\n",
    "    edited_sentence = sentence\n",
    "    while edited_sentence == sentence: # to ensure each sentence contains at least one spelling error\n",
    "        edited_sentence = [generate_misspelling(word) for word in sentence.split()]\n",
    "        edited_sentence = ' '.join(edited_sentence)\n",
    "    test_set.append([edited_sentence, sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: bigram\n",
      "accuracy: 0.32\n",
      "method: norvig\n",
      "accuracy: 0.32666666666666666\n"
     ]
    }
   ],
   "source": [
    "def test(test_set, method):\n",
    "    correct = 0\n",
    "    for misspelled_sentence, correct_sentence in test_set:\n",
    "        corrected_sentence = corrector(misspelled_sentence, method=method)\n",
    "        if corrected_sentence == correct_sentence:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(test_set)\n",
    "    print(\"method: \" + method)\n",
    "    print(\"accuracy: \" + str(accuracy))\n",
    "    return\n",
    "        \n",
    "test(test_set, method=\"bigram\")\n",
    "test(test_set, method=\"norvig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
